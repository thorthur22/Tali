--- a/src/tali/retrieval.py
+++ b/src/tali/retrieval.py
@@ -73,6 +73,25 @@
             rows = self.db.search_facts(user_input, self.config.max_facts)
         if not rows:
             rows = self.db.list_facts()
+
+        # Poison-resistant filters:
+        # - Exclude contested facts unless they were directly retrieved by vector similarity.
+        # - Exclude low-confidence inferred facts unless they were directly retrieved by vector similarity.
+        direct_ids = {hit.item_id for hit in vector_hits if hit.item_type == "fact"}
+        filtered_rows = []
+        for row in rows:
+            is_direct = row["id"] in direct_ids
+            if row["contested"] and not is_direct:
+                continue
+            if (
+                row["provenance_type"] == ProvenanceType.INFERRED.value
+                and row["confidence"] < 0.5
+                and not is_direct
+            ):
+                continue
+            filtered_rows.append(row)
+        rows = filtered_rows
+
         ranked = sorted(
             rows,
             key=lambda row: (
--- a/src/tali/db.py
+++ b/src/tali/db.py
@@ -333,3 +333,38 @@
                 """,
                 (run_id, timestamp, last_episode_timestamp),
             )
+
+
+    def insert_hypothesis(
+        self,
+        hypothesis_id: str,
+        statement: str,
+        origin_episode_id: str,
+        confidence: float,
+        status: str,
+        created_at: str,
+    ) -> None:
+        with self.connect() as connection:
+            connection.execute(
+                """
+                INSERT INTO hypotheses (id, statement, origin_episode_id, confidence, status, created_at)
+                VALUES (?, ?, ?, ?, ?, ?)
+                """,
+                (hypothesis_id, statement, origin_episode_id, confidence, status, created_at),
+            )
+
+    def apply_fact_decay(self, cutoff_timestamp: str) -> int:
+        """Apply confidence decay to facts not confirmed since cutoff_timestamp.
+
+        Returns the number of rows updated.
+        """
+        with self.connect() as connection:
+            cursor = connection.execute(
+                """
+                UPDATE facts
+                SET confidence = MAX(0.0, confidence - decay_rate)
+                WHERE COALESCE(last_confirmed, created_at) < ?
+                """,
+                (cutoff_timestamp,),
+            )
+            return cursor.rowcount
--- a/src/tali/consolidation.py
+++ b/src/tali/consolidation.py
@@ -3,7 +3,7 @@
 import re
 import uuid
 from dataclasses import dataclass
-from datetime import datetime
+from datetime import datetime, timedelta
 
 from tali.db import Database
 from tali.models import FORBIDDEN_FACT_TYPES, ProvenanceType
@@ -23,6 +23,17 @@
     inserted_fact_ids: list[str]
     skipped_candidates: list[str]
     contested_fact_ids: list[str]
+
+def can_promote_fact(provenance_type: str, source_ref: str, confidence: float) -> bool:
+    """Central gate for whether a candidate may be persisted as a Fact."""
+    if not provenance_type or not source_ref:
+        return False
+    if provenance_type in FORBIDDEN_FACT_TYPES:
+        return False
+    # Inferred items are treated as hypotheses by default until reinforced/verified.
+    if provenance_type == ProvenanceType.INFERRED.value and confidence < 0.5:
+        return False
+    return True
 
 
 def apply_sleep_output(db: Database, payload: dict[str, object]) -> ConsolidationResult:
@@ -45,14 +56,26 @@
         if not statement or not provenance_type or not source_ref:
             skipped_candidates.append("missing_fields")
             continue
-        if provenance_type in FORBIDDEN_FACT_TYPES:
-            skipped_candidates.append("forbidden_provenance")
-            continue
         if provenance_type not in CONFIDENCE_DEFAULTS:
             skipped_candidates.append("unknown_provenance")
             continue
+        default_conf = CONFIDENCE_DEFAULTS[str(provenance_type)]
         if not db.episode_exists(str(source_ref)):
             skipped_candidates.append("missing_source")
+            continue
+
+        # Central promotion gate: inferred items remain hypotheses until reinforced/verified.
+        if not can_promote_fact(str(provenance_type), str(source_ref), float(default_conf)):
+            hypothesis_id = str(uuid.uuid4())
+            db.insert_hypothesis(
+                hypothesis_id=hypothesis_id,
+                statement=str(statement),
+                origin_episode_id=str(source_ref),
+                confidence=float(default_conf),
+                status="active",
+                created_at=datetime.utcnow().isoformat(),
+            )
+            skipped_candidates.append("stored_as_hypothesis")
             continue
 
         existing = db.search_facts(str(statement), limit=5)
@@ -74,7 +97,7 @@
             statement=str(statement),
             provenance_type=str(provenance_type),
             source_ref=str(source_ref),
-            confidence=CONFIDENCE_DEFAULTS[str(provenance_type)],
+            confidence=float(default_conf),
             created_at=created_at,
             contested=contested,
         )
@@ -92,6 +115,10 @@
                     created_at=datetime.utcnow().isoformat(),
                 )
                 db.mark_fact_contested(related_id)
+
+    # Apply conservative confidence decay to reduce stale memory impact.
+    cutoff = (datetime.utcnow() - timedelta(days=30)).isoformat()
+    db.apply_fact_decay(cutoff)
 
     return ConsolidationResult(
         inserted_fact_ids=inserted_fact_ids,
@@ -120,4 +147,4 @@
         return ("not" in statement_tokens) != ("not" in existing_tokens) or (
             "never" in statement_tokens
         ) != ("never" in existing_tokens) or ("no" in statement_tokens) != ("no" in existing_tokens)
-    return False
+    return False--- a/src/tali/sleep.py
+++ b/src/tali/sleep.py
@@ -1,7 +1,6 @@
 from __future__ import annotations
 
 import json
-import re
 from dataclasses import dataclass
 from datetime import datetime
 from pathlib import Path
@@ -90,13 +89,14 @@
 
 
 def _parse_json(text: str) -> dict[str, object]:
+    """Parse strict JSON from the model.
+
+    This fails closed: if the model does not return valid JSON, sleep aborts.
+    """
     try:
         payload = json.loads(text)
-    except json.JSONDecodeError:
-        match = re.search(r"\{.*\}", text, re.DOTALL)
-        if not match:
-            raise
-        payload = json.loads(match.group(0))
+    except json.JSONDecodeError as e:
+        raise ValueError("Sleep output must be strict JSON with no extra text.") from e
     if not isinstance(payload, dict):
         raise ValueError("Sleep output must be a JSON object.")
     return payload
